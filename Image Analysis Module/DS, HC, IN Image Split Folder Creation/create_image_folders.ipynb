{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download necessary dependencies to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m\u001b[1mInstalling dependencies from Pipfile.lock (f5fbd1)...\u001b[39m\u001b[22m\n",
      "  üêç   \u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m\u001b[32m\u001b[1m‚ñâ\u001b[39m\u001b[22m 0/0 ‚Äî \u001b[30m\u001b[22m00:00:00\u001b[39m\u001b[22m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pipenv install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from os.path import join, splitext\n",
    "import requests\n",
    "import shutil\n",
    "import argparse\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download & extract data for disaster types, damage severity, humanitarian categories, and informativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccid_tarfile_link = \"https://crisisnlp.qcri.org/data/crisis_image_datasets_benchmarks/crisis_vision_benchmarks.tar.gz\"\n",
    "ccid_data_tar_filename = ccid_tarfile_link.split('/')[-1]\n",
    "p = Path(ccid_data_tar_filename)\n",
    "extensions = \"\".join(p.suffixes)\n",
    "ccid_data_name = str(p).replace(extensions, \"\")\n",
    "full_ccid_data_dir_path = join('.', ccid_data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading & Extracting crisis_vision_benchmarks.tar.gz as directory ./crisis_vision_benchmarks ...\n",
      "Extracted crisis_vision_benchmarks.tar.gz as directory ./crisis_vision_benchmarks\n"
     ]
    }
   ],
   "source": [
    "print(f'Downloading & Extracting {ccid_data_tar_filename} as directory {full_ccid_data_dir_path} ...')\n",
    "response = requests.get(ccid_tarfile_link, stream = True)\n",
    "file = tarfile.open(fileobj = response.raw, mode = \"r|gz\")\n",
    "file.extractall(path = '.')\n",
    "file.close()\n",
    "print(f'Extracted {ccid_data_tar_filename} as directory {full_ccid_data_dir_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_path = join(full_ccid_data_dir_path, 'tasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_tsvs = {\n",
    "    'damage_severity': join(tasks_path, 'damage_severity'),\n",
    "    'humanitarian_categories': join(tasks_path, 'humanitarian'),\n",
    "    'informativeness': join(tasks_path, 'informative')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "EVENT_NAME_COL_NAME, CLASS_LABEL_COL_NAME = 'event_name', 'class_label'\n",
    "IMAGE_PATH_COL_NAME = 'image_path'\n",
    "IMAGE_ID_COL_NAME = 'image_id'\n",
    "NEW_IMG_PATH_COL_NAME = 'new_path'\n",
    "OLD_IMG_PATH_COL_NAME = 'old_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_labeled_data_folder(parent_path, data_parent_path, task_folder_path, folder_name):\n",
    "    '''\n",
    "    Constructs image data folder that is separated into train, dev, and test split folders\n",
    "    which are then separated into folders for each of the labels in class_labels.\n",
    "    \n",
    "            Parameters:\n",
    "                    parent_path (string): path of the parent directory for where we will create this folder\n",
    "                    data_parent_path (string): path to the directory containing the folder which contains the source images\n",
    "                    task_folder_path (string): path of the task folder we would like to extract labeled images from\n",
    "                    folder_name(string): name of the folder we wish to create with this function\n",
    "\n",
    "    '''\n",
    "    task_name = task_folder_path.split('/')[-1]\n",
    "    \n",
    "    task_tsvs_path = join(task_folder_path, 'consolidated')\n",
    "    tsv_paths = os.listdir(task_tsvs_path)\n",
    "    split_filenames_dict = {split: list(filter(lambda filename: split in filename, tsv_paths))[0] for split in SPLITS}\n",
    "    split_tsv_paths = {split: join(task_tsvs_path, split_filenames_dict[split]) for split in SPLITS}\n",
    "    split_dfs = {split: pd.read_csv(split_tsv_paths[split], sep='\\t')for split in SPLITS}\n",
    "    \n",
    "    class_labels = split_dfs['train'][CLASS_LABEL_COL_NAME].unique()\n",
    "    folder_path = join(parent_path, folder_name)\n",
    "    os.mkdir(folder_path)\n",
    "    # Make train/val/test directories and class labeled directories if they don't already exist\n",
    "    for split in SPLITS:\n",
    "        filenames = {} # To help with duplicate image filenames\n",
    "        split_df = split_dfs[split]\n",
    "        split_df[OLD_IMG_PATH_COL_NAME] = split_df[IMAGE_PATH_COL_NAME]\n",
    "        split_df[NEW_IMG_PATH_COL_NAME] = np.nan\n",
    "        split_df = split_df[[EVENT_NAME_COL_NAME, IMAGE_ID_COL_NAME, OLD_IMG_PATH_COL_NAME, NEW_IMG_PATH_COL_NAME, CLASS_LABEL_COL_NAME]].copy()\n",
    "        split_path = join(folder_path, split)\n",
    "        os.mkdir(split_path)\n",
    "        for label in class_labels:\n",
    "            label_path = join(split_path, label)\n",
    "            os.mkdir(label_path)\n",
    "            label_df = split_df[split_df[CLASS_LABEL_COL_NAME] == label]\n",
    "            for index, row in label_df.iterrows():\n",
    "                src_event = row[EVENT_NAME_COL_NAME]\n",
    "                abs_img_path = row[OLD_IMG_PATH_COL_NAME]\n",
    "                rel_img_path = join(data_parent_path, abs_img_path)\n",
    "                image_name = abs_img_path.split('/')[-1]\n",
    "                if image_name in filenames: # Found duplicate image filename\n",
    "                    filenames[image_name] += 1\n",
    "                    filename, ext = splitext(image_name)\n",
    "                    image_name = filename + '_' + str(filenames[image_name]) + ext\n",
    "                    filenames[image_name] = 1\n",
    "                else:\n",
    "                    filenames[image_name] = 1\n",
    "                final_img_path = join(label_path, image_name)\n",
    "                abs_final_img_path = \"/\".join(final_img_path.split('/')[1:])\n",
    "                split_df.loc[index] = [src_event, image_name, abs_img_path, abs_final_img_path, row[CLASS_LABEL_COL_NAME]]\n",
    "                shutil.copy(rel_img_path, final_img_path)\n",
    "        split_filename = split + '.csv'\n",
    "        split_df.to_csv(join(folder_path, split_filename), index = False)\n",
    "        print(f\"Number of samples in the {split} set for the {task_name} task: {len(split_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccid_splits_folder = join('.', 'CCID Splits Data')\n",
    "os.mkdir(ccid_splits_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Train/Dev/Test Image Folders for damage_severity task located at ./CCID Splits Data/damage_severity\n",
      "Number of samples in the train set for the damage_severity task: 28319\n",
      "Number of samples in the dev set for the damage_severity task: 2712\n",
      "Number of samples in the test set for the damage_severity task: 3865\n",
      "Completed creating Train/Dev/Test Image Folders for damage_severity task located at ./CCID Splits Data/damage_severity.\n",
      "\n",
      "Creating Train/Dev/Test Image Folders for humanitarian_categories task located at ./CCID Splits Data/humanitarian_categories\n",
      "Number of samples in the train set for the humanitarian task: 12618\n",
      "Number of samples in the dev set for the humanitarian task: 1229\n",
      "Number of samples in the test set for the humanitarian task: 2922\n",
      "Completed creating Train/Dev/Test Image Folders for humanitarian_categories task located at ./CCID Splits Data/humanitarian_categories.\n",
      "\n",
      "Creating Train/Dev/Test Image Folders for informativeness task located at ./CCID Splits Data/informativeness\n",
      "Number of samples in the train set for the informative task: 48186\n",
      "Number of samples in the dev set for the informative task: 3054\n",
      "Number of samples in the test set for the informative task: 8477\n",
      "Completed creating Train/Dev/Test Image Folders for informativeness task located at ./CCID Splits Data/informativeness.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task_name, task_tsv_path in paths_to_tsvs.items():\n",
    "    print(f'Creating Train/Dev/Test Image Folders for {task_name} task located at {join(ccid_splits_folder, task_name)}')\n",
    "    construct_labeled_data_folder(ccid_splits_folder, full_ccid_data_dir_path, task_tsv_path, task_name)\n",
    "    print(f'Completed creating Train/Dev/Test Image Folders for {task_name} task located at {join(ccid_splits_folder, task_name)}.')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccid-test",
   "language": "python",
   "name": "ccid-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
