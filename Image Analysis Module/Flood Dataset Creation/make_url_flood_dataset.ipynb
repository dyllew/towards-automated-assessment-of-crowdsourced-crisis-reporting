{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL Flood Presence Dataset Creation Notebook\n",
    "#### Constructs Dataset, Train/Dev/Test Data Splits and corresponding folders containing images.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE:**\n",
    "\n",
    "Some of the tweet data we use is produced from rehydrating tweets from tweet IDs. Because of this, some tweets may have been deleted since the work conducted in the [original work](https://github.com/dyllew/towards-automated-crowdsourced-crisis-reporting) which created the first version of the URL Flood Presence dataset and Train/Dev/Test splits. If you would like to access to the original tweets used to create the original URL Flood Presence dataset as was used in the [original work](https://github.com/dyllew/towards-automated-crowdsourced-crisis-reporting), please email [url_googleai@mit.edu](mailto:url_googleai@mit.edu) with your request for the URL Flood Presence Dataset and your plans for using it. We can only permit use for non-commercial research in accordance with [Twitter's Content Redistribution Policies](https://developer.twitter.com/en/developer-terms/agreement-and-policy) and you must comply with the Twitter Terms of Service, Privacy Policy, Development Agreement, and Developer Policy prior to receiving the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, splitext, exists\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n",
    "from copy import copy\n",
    "import requests\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from git.repo.base import Repo\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EVENT_NAME_COL_NAME, CLASS_LABEL_COL_NAME = 'event_name', 'class_label'\n",
    "FLOOD_LABEL_NAME, NOT_FLOOD_LABEL_NAME = 'flood', 'not_flood'\n",
    "LABELS_LIST = [FLOOD_LABEL_NAME, NOT_FLOOD_LABEL_NAME]\n",
    "IMAGE_PATH_COL_NAME = 'image_path'\n",
    "\n",
    "def make_dir(dir_path):\n",
    "    try:\n",
    "        os.mkdir(dir_path)\n",
    "    except FileExistsError:\n",
    "        print('{} directory already exists.'.format(dir_path))\n",
    "\n",
    "def print_label_totals(df, df_name):\n",
    "    # Allows us to see number of data points for each class (Flood/Not Flood) from each event source\n",
    "    event_names = df[EVENT_NAME_COL_NAME].unique()\n",
    "    labels = df[CLASS_LABEL_COL_NAME].unique()\n",
    "    total_label_counts = {label:0 for label in labels}\n",
    "    for event_name in event_names:\n",
    "        event_label_counts = {label:0 for label in labels}\n",
    "        for label in labels:\n",
    "            mask = (df[EVENT_NAME_COL_NAME] == event_name) & (df[CLASS_LABEL_COL_NAME] == label) & (df[IMAGE_PATH_COL_NAME].apply(exists))\n",
    "            num_label = len(df[mask])\n",
    "            event_label_counts[label] = num_label\n",
    "            total_label_counts[label] += num_label\n",
    "        print(f'{event_name}: ')\n",
    "        total_event_imgs = len(df[df[EVENT_NAME_COL_NAME] == event_name])\n",
    "        for label in labels:\n",
    "            print(f'    {label}: {event_label_counts[label]} images')\n",
    "        print(f'    Total: {total_event_imgs} images')\n",
    "    df_length = len(df)\n",
    "    print(f'{df_name}:' )\n",
    "    for label in labels:\n",
    "        print(f'    {label}: {total_label_counts[label]} images')\n",
    "    print(f'    Total: {df_length} images')\n",
    "    \n",
    "\n",
    "def add_imgs_to_df(df, img_name_set, src_dir, data_dir_path, event_name, label):\n",
    "    src_dir_img_names = os.listdir(src_dir)\n",
    "    cant_find_count = 0\n",
    "    for img_name in img_name_set:\n",
    "        i = len(df)\n",
    "        try:\n",
    "            original_filename = list(filter(lambda x: splitext(x)[0] == img_name, src_dir_img_names))[0]\n",
    "            full_img_path = join(data_dir_path, original_filename)\n",
    "            df.loc[i] = [event_name, original_filename, full_img_path, label]\n",
    "        except IndexError:\n",
    "            # If file does not exist, we don't add it to the dataframe\n",
    "            cant_find_count += 1\n",
    "    print(\"Could not find {} {} images from the {} dataset\".format(cant_find_count, label, event_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets path to directory for URL Flood presence dataset and Train/Dev/Test splits\n",
    "flood_dataset_path = './Flood Dataset'\n",
    "make_dir(flood_dataset_path)\n",
    "final_splits_dir_path = join(flood_dataset_path, 'url_flood_presence')\n",
    "make_dir(final_splits_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\n",
    "    'train',\n",
    "    'dev',\n",
    "    'test'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to aggregate data from three sources to make this dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Consolidated Crisis Dataset [1, 2, 3] - Disaster Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We construct the labeled directories Flood vs. Not Flood derived from the Consolidated Crisis Image data set\n",
    "The data for the original task was disaster type classification. We use the **\"flood\"** class from the original dataset for the positive **\"flood\"** class in our case and consolidate all other classes from the original dataset into the negative **\"not flood\"** class.\n",
    "Link to this data is [here](https://crisisnlp.qcri.org/crisis-image-datasets-asonam20#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the Train/Dev/Test splits provided by the authors, and we consolidate the labels into **\"flood\"** and **\"not_flood\"** categories to be used to construct the labeled image folders for our case. We begin by constructing a folder of all the images from the original dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Download and Extract tarfile containing images and Train/Dev/Test splits for Disaster Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_types_tarfile_link = \"https://crisisnlp.qcri.org/data/crisis_image_datasets_benchmarks/data_disaster_types.tar.gz\"\n",
    "disaster_types_data_tar_filename = disaster_types_tarfile_link.split('/')[-1]\n",
    "p = Path(disaster_types_data_tar_filename)\n",
    "extensions = \"\".join(p.suffixes)\n",
    "disaster_types_data_name = str(p).replace(extensions, \"\")\n",
    "full_disaster_types_data_dir_path = join(flood_dataset_path, disaster_types_data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Downloading & Extracting {disaster_types_data_tar_filename} as directory {full_disaster_types_data_dir_path} ...')\n",
    "response = requests.get(disaster_types_tarfile_link, stream = True)\n",
    "file = tarfile.open(fileobj = response.raw, mode = \"r|gz\")\n",
    "file.extractall(path = flood_dataset_path)\n",
    "file.close()\n",
    "print(f'Extracted {disaster_types_data_tar_filename} as directory {full_disaster_types_data_dir_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Using Train/Dev/Test splits relabel using labeling scheme described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_types_splits_dict = {\n",
    "   split: pd.read_csv(join(full_disaster_types_data_dir_path, f'consolidated_disaster_types_{split}_final.tsv'), sep='\\t') for split in splits\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = disaster_types_splits_dict[splits[0]].columns\n",
    "for split in splits:\n",
    "    df = disaster_types_splits_dict[split]\n",
    "    df[CLASS_LABEL_COL_NAME] = df[CLASS_LABEL_COL_NAME].apply(lambda x: FLOOD_LABEL_NAME if x == FLOOD_LABEL_NAME else NOT_FLOOD_LABEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_string_from_path(df, path_col_name, bad_string, path_idx):\n",
    "    new_df = df.copy()\n",
    "    def path_func(path, bad_string, path_idx):\n",
    "        split_path = path.split('/')\n",
    "        if split_path[path_idx] == bad_string:\n",
    "            mod_path = \"/\".join(split_path[:path_idx] + split_path[path_idx+1:])\n",
    "            return mod_path\n",
    "        return path\n",
    "    new_df[path_col_name] = new_df[path_col_name].apply(lambda path: path_func(path, bad_string, path_idx))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Create labeled flood/not_flood CSVs for disaster types CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_STRING = 'aidr_disaster_types'\n",
    "for split in splits:\n",
    "    split_df = disaster_types_splits_dict[split]\n",
    "    split_df = remove_bad_string_from_path(split_df, IMAGE_PATH_COL_NAME, BAD_STRING, 1)\n",
    "    print(f'disaster types {split} split has {len(split_df)} total images')\n",
    "    abs_disaster_types_data_dir_path = \"/\".join(full_disaster_types_data_dir_path.split(\"/\")[1:])\n",
    "    split_df[IMAGE_PATH_COL_NAME] = split_df[IMAGE_PATH_COL_NAME].apply(lambda path: join(abs_disaster_types_data_dir_path, path))\n",
    "    # Subset to files which actually exist\n",
    "    split_df = split_df[split_df[IMAGE_PATH_COL_NAME].apply(exists)]\n",
    "    # Save splits\n",
    "    split_csv_path = join(flood_dataset_path, f'consolidated_disaster_types_{split}.csv')\n",
    "    split_df.to_csv(split_csv_path, index = False)\n",
    "    print_label_totals(split_df, f'Full {split}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Central European Floods 2013 [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We construct the dataframe with labels \"flood\" and \"not_flood\" from the EU flood data set which consists of flood-relevant and flood-irrelevant images from central European floods of May & June 2013\n",
    "Link to this data repo is [here](https://github.com/cvjena/eu-flood-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_flood_repo_url = \"https://github.com/cvjena/eu-flood-dataset\"\n",
    "eu_flood_data_path = join(flood_dataset_path, 'eu_flood_data')\n",
    "make_dir(eu_flood_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Repo.clone_from(eu_flood_repo_url, eu_flood_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate photos for labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_path = join(eu_flood_data_path, 'relevance')\n",
    "relevance_dict = {}\n",
    "relevance_files_list = os.listdir(relevance_path)\n",
    "for relevance_filename in relevance_files_list:\n",
    "    label_name = os.path.splitext(relevance_filename)[0]\n",
    "    path_data_filename = join(relevance_path, relevance_filename)\n",
    "    with open(path_data_filename, 'r') as f:\n",
    "        relevance_dict[label_name] = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_relevant_data_filenames = set(relevance_dict['flooding'])\n",
    "depth_relevant_data_filenames = set(relevance_dict['depth'])\n",
    "pollution_relevant_data_filenames = set(relevance_dict['pollution'])\n",
    "irrelevant_data_filenames = set(relevance_dict['irrelevant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_flood_data_filenames = set()\n",
    "not_flood_data_filenames.update(irrelevant_data_filenames)\n",
    "not_flood_data_filenames.update(depth_relevant_data_filenames - flood_relevant_data_filenames)\n",
    "not_flood_data_filenames.update(pollution_relevant_data_filenames - flood_relevant_data_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download imgs_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_img_small_url = \"https://archive.org/download/european-flood-2013/european-flood-2013_imgs_small.zip\"\n",
    "eu_img_dir = join(eu_flood_data_path, 'imgs_small')\n",
    "filename = eu_img_small_url.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def bar_progress(current, total, width=80):\n",
    "  progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
    "  # Don't use print() as it will print in new line every time.\n",
    "  sys.stdout.write(\"\\r\" + progress_message)\n",
    "  sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Downloading {filename}')\n",
    "wget.download(eu_img_small_url, bar=bar_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract imgs_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfile = ZipFile(filename)\n",
    "zfile.extractall(eu_flood_data_path)\n",
    "zfile.close()\n",
    "print(f'Extracted {filename} as directory {eu_img_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label images using \"flood\" & \"not_flood\" labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_data_df = pd.DataFrame(columns=data_columns)\n",
    "eu_data_abs_dir_path = '/'.join(eu_img_dir.split('/')[1:])\n",
    "eu_event_name = 'central_eu13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_imgs_to_df(eu_data_df, flood_relevant_data_filenames, eu_img_dir, eu_data_abs_dir_path, eu_event_name, FLOOD_LABEL_NAME)\n",
    "add_imgs_to_df(eu_data_df, not_flood_data_filenames, eu_img_dir, eu_data_abs_dir_path, eu_event_name, NOT_FLOOD_LABEL_NAME)\n",
    "\n",
    "num_flood_imgs = len(eu_data_df[eu_data_df[CLASS_LABEL_COL_NAME] == FLOOD_LABEL_NAME])\n",
    "num_not_flood_imgs = len(eu_data_df[eu_data_df[CLASS_LABEL_COL_NAME] == NOT_FLOOD_LABEL_NAME])\n",
    "\n",
    "eu_csv_path = join(flood_dataset_path, 'eu_labeled_flood_data.csv')\n",
    "eu_data_df.to_csv(eu_csv_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_label_totals(eu_data_df, eu_event_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Harz 2017 & Rhine 2018 [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We construct the labeled image folders flood vs. not_flood for the Twitter scrape of flood-related keywords of the Harz17 and Rhine18 flood events in Germany. [5] \n",
    "Link to the data repo is [here](https://github.com/cvjena/twitter-flood-dataset)\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "Since this data source is made from the rehydration of tweets using tweet IDs, some of the original 704 tweets for Harz 2017 and 1848 tweets for Rhine 2018 may have been deleted and thus can no longer be used for adding to the URL Flood Presence Dataset. Please see the note at the top of the notebook for directions for attaining the original URL Flood Presence Dataset used in the [original work](https://github.com/dyllew/towards-automated-crowdsourced-crisis-reporting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_flood_repo_url = \"https://github.com/cvjena/twitter-flood-dataset\"\n",
    "path_twitter_flood_data = join(flood_dataset_path, 'twitter_flood_data')\n",
    "path_to_download_file = join(path_twitter_flood_data, 'download_images.py')\n",
    "harz17_dir = join(path_twitter_flood_data, 'harz17')\n",
    "rhine18_dir = join(path_twitter_flood_data, 'rhine18')\n",
    "make_dir(path_twitter_flood_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Repo.clone_from(twitter_flood_repo_url, path_twitter_flood_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good to run the cell below a few times to ensure you get maximum number of currently existant tweets from the original dataset Harz17 & Rhine18 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '{path_to_download_file}' '{harz17_dir}' '{rhine18_dir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_name in ['harz17', 'rhine18']:\n",
    "    data_dir_files = os.listdir(path_twitter_flood_data)\n",
    "    data_img_dir = join(path_twitter_flood_data, event_name)\n",
    "    data_filenames = os.listdir(data_img_dir)\n",
    "    data_json_path = join(path_twitter_flood_data, '{}.json'.format(event_name))\n",
    "    img_dict_list = []\n",
    "    event_data_df = pd.DataFrame(columns=data_columns)\n",
    "    with open(data_json_path) as data_json_file:\n",
    "        json_data = json.load(data_json_file)\n",
    "        for img_name in json_data.keys():\n",
    "            img_dict = copy(json_data[img_name])\n",
    "            img_dict['image_filename'] = img_name\n",
    "            img_dict_list.append(img_dict)\n",
    "    flood_filenames = set(map(lambda img_dict: img_dict['image_filename'], filter(lambda img_dict: img_dict['RelFlooding'], img_dict_list)))\n",
    "    not_flood_filenames = set(map(lambda img_dict: img_dict['image_filename'], filter(lambda img_dict: not img_dict['RelFlooding'], img_dict_list)))\n",
    "    event_data_dir_path = '/'.join(data_img_dir.split('/')[1:])\n",
    "    add_imgs_to_df(event_data_df, flood_filenames, data_img_dir, event_data_dir_path, event_name, FLOOD_LABEL_NAME)\n",
    "    add_imgs_to_df(event_data_df, not_flood_filenames, data_img_dir, event_data_dir_path, event_name, NOT_FLOOD_LABEL_NAME)\n",
    "    event_csv_filename = '{}_labeled_flood_data.csv'.format(event_name)\n",
    "    event_csv_path = join(flood_dataset_path, event_csv_filename)\n",
    "    event_data_df.to_csv(event_csv_path, index = False)\n",
    "    print_label_totals(event_data_df, event_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Final Train/Dev/Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the data prepared into separate folders, we need to construct our own Train/Dev/Test splits based on the consolidated train / dev / test splits, we use the sci-kit learn train/dev/test split function on the EU and twitter data and append these entries to the consolidated train/dev/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_eu13_df = pd.read_csv(join(flood_dataset_path, 'eu_labeled_flood_data.csv'))\n",
    "harz17_df = pd.read_csv(join(flood_dataset_path, 'harz17_labeled_flood_data.csv'))\n",
    "rhine18_df = pd.read_csv(join(flood_dataset_path, 'rhine18_labeled_flood_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed for reproducible randomized splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "train_ratio, dev_ratio, test_ratio = 0.7, 0.1, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_flood_data = pd.concat([central_eu13_df, harz17_df, rhine18_df], ignore_index=True)\n",
    "X = other_flood_data.drop(columns=[CLASS_LABEL_COL_NAME])\n",
    "y = other_flood_data[CLASS_LABEL_COL_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_ratio, random_state = seed)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(\n",
    "    X_train, y_train, test_size = dev_ratio/(train_ratio + dev_ratio), random_state = seed\n",
    ")\n",
    "created_splits_data = {\n",
    "    'train': (X_train, y_train),\n",
    "    'dev': (X_dev, y_dev),\n",
    "    'test': (X_test, y_test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_flood_data_dfs_dict = {}\n",
    "for split in splits:\n",
    "    (X_split, y_split) = created_splits_data[split]\n",
    "    split_df = X_split\n",
    "    split_df[CLASS_LABEL_COL_NAME] = y_split\n",
    "    other_flood_data_dfs_dict[split] = split_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_data_dfs_dict = {\n",
    "    split: pd.read_csv(join(flood_dataset_path, f'consolidated_disaster_types_{split}.csv')) for split in splits\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_split_dfs_dict = {\n",
    "    split:pd.concat([consolidated_data_dfs_dict[split], other_flood_data_dfs_dict[split]], ignore_index = True) for split in splits\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Train/Dev/Test splits (as .csv) using full data sources and create folders containing images in their corresponding Train/Dev/Test Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, split_df in final_split_dfs_dict.items():\n",
    "    filenames = {} # To help with duplicate image filenames\n",
    "    split_name = 'url_flood_presence_{}'.format(split)\n",
    "    split_path = join(final_splits_dir_path, split)\n",
    "    make_dir(split_path)\n",
    "    labels = split_df[CLASS_LABEL_COL_NAME].unique()\n",
    "    for label in labels:\n",
    "        label_path = join(split_path, label)\n",
    "        make_dir(label_path)\n",
    "        label_df = split_df[split_df[CLASS_LABEL_COL_NAME] == label]\n",
    "        for index, row in label_df.iterrows():\n",
    "            src_event = row[EVENT_NAME_COL_NAME]\n",
    "            abs_img_path = row[IMAGE_PATH_COL_NAME]\n",
    "            image_name = abs_img_path.split('/')[-1]\n",
    "            if image_name in filenames: # Found duplicate image filename\n",
    "                filenames[image_name] += 1\n",
    "                filename, ext = splitext(image_name)\n",
    "                image_name = filename + '_' + str(filenames[image_name]) + ext\n",
    "                filenames[image_name] = 1\n",
    "            else:\n",
    "                filenames[image_name] = 1\n",
    "            final_img_path = join(label_path, image_name)\n",
    "            abs_final_img_path = \"/\".join(final_img_path.split('/')[1:])\n",
    "            split_df.loc[index] = [row[EVENT_NAME_COL_NAME], image_name, abs_final_img_path, row[CLASS_LABEL_COL_NAME]]\n",
    "            shutil.copy(abs_img_path, final_img_path)\n",
    "    print_label_totals(split_df, split_name)\n",
    "    split_filename = split_name + '.csv'\n",
    "    split_df.to_csv(join(final_splits_dir_path, split_filename), index = False)\n",
    "print()\n",
    "print(f'Final URL Flood Presence Train/Dev/Test splits and corresponding image folders can be found in {final_splits_dir_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "[1] ***Firoj Alam, Ferda Ofli, Muhammad Imran, Tanvirul Alam, Umair Qazi, [Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response](https://arxiv.org/pdf/2011.08916.pdf), In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), 2020.***\n",
    "```\n",
    "@inproceedings{crisisdataset2020-images,\n",
    "Author = {Firoj Alam and Ferda Ofli and Muhammad Imran and Tanvirul Alam and Umair Qazi},\n",
    "Keywords = {Social Media, Crisis Computing, Tweet Text Classification, Disaster Response},\n",
    "Title = {Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response},\n",
    "Publisher = {IEEE},\n",
    "Booktitle = {2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},\n",
    "Year = {2020}\n",
    "}\n",
    "```\n",
    "\n",
    "[2] ***Firoj Alam, Ferda Ofli, and Muhammad Imran, CrisisMMD: Multimodal Twitter Datasets from Natural Disasters. In Proceedings of the 12th International AAAI Conference on Web and Social Media (ICWSM), 2018, Stanford, California, USA.***\n",
    "```\n",
    "@InProceedings{crisismmd,\n",
    "  author = {Alam, Firoj and Ofli, Ferda and Imran, Muhammad},\n",
    "  title = { CrisisMMD: Multimodal Twitter Datasets from Natural Disasters},\n",
    "  booktitle = {Proceedings of the 12th International AAAI Conference on Web and Social Media (ICWSM)},\n",
    "  year = {2018},\n",
    "  month = {June},\n",
    "  date = {23-28},\n",
    "  location = {USA}}\n",
    "```\n",
    "[3] ***Hussein Mozannar, Yara Rizk, and Mariette Awad, Damage Identification in Social Media Posts using Multimodal Deep Learning, In Proc. of ISCRAM, May 2018, pp. 529–543***\n",
    "```\n",
    " @inproceedings{multimodal-deep-learning, title={Damage Identification in Social Media Posts using Multimodal Deep Learning}, booktitle={ISCRAM 2018 Conference Proceedings – 15th International Conference on Information Systems for Crisis Response and Management}, author={Mouzannar, Hussein and Yara Rizk and Awad, Mariette}, year={2018}, pages={529--543}} \n",
    "```\n",
    "\n",
    "[4] ***Björn Barz, Kai Schröter, Moritz Münch, Bin Yang, Andrea Unger, Doris Dransch, and Joachim Denzler.\n",
    "\"Enhancing Flood Impact Analysis using Interactive Image Retrieval of Social Media Images.\"\n",
    "Archives of Data Science, Series A, 5.1, 2019.***\n",
    "```\n",
    "@article{flood-impact-in-european-context,\n",
    "  author    = {Bj{\\\"{o}}rn Barz and\n",
    "               Kai Schr{\\\"{o}}ter and\n",
    "               Moritz M{\\\"{u}}nch and\n",
    "               Bin Yang and\n",
    "               Andrea Unger and\n",
    "               Doris Dransch and\n",
    "               Joachim Denzler},\n",
    "  title     = {Enhancing Flood Impact Analysis using Interactive Retrieval of Social\n",
    "               Media Images},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/1908.03361},\n",
    "  year      = {2019},\n",
    "  url       = {http://arxiv.org/abs/1908.03361},\n",
    "  eprinttype = {arXiv},\n",
    "  eprint    = {1908.03361},\n",
    "  timestamp = {Mon, 19 Aug 2019 13:21:03 +0200},\n",
    "  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-03361.bib},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "```\n",
    "\n",
    "[5] ***Björn Barz, Kai Schröter, Ann-Christin Kra, and Joachim Denzler.\n",
    "\"Finding Relevant Flood Images on Twitter using Content-based Filters.\"\n",
    "ICPR 2020 Workshop on Machine Learning Advances Environmental Science.***\n",
    "```\n",
    "@article{flood-impact-euro-context-twitter,\n",
    "  author    = {Bj{\\\"{o}}rn Barz and\n",
    "               Kai Schr{\\\"{o}}ter and\n",
    "               Ann{-}Christin Kra and\n",
    "               Joachim Denzler},\n",
    "  title     = {Finding Relevant Flood Images on Twitter using Content-based Filters},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/2011.05756},\n",
    "  year      = {2020},\n",
    "  url       = {https://arxiv.org/abs/2011.05756},\n",
    "  eprinttype = {arXiv},\n",
    "  eprint    = {2011.05756},\n",
    "  timestamp = {Thu, 12 Nov 2020 15:14:56 +0100},\n",
    "  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-05756.bib},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood-dataset-creation-v2",
   "language": "python",
   "name": "flood-dataset-creation-v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
